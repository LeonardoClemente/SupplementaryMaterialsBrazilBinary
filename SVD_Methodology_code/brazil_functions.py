import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
import copy
import time
from sklearn.utils.extmath import randomized_svd as svd
from scipy.spatial import ConvexHull
from scipy.spatial import Delaunay
from shapely.geometry import Polygon
from scipy.spatial import distance
from detect_peaks import detect_peaks
import pickle
from sklearn import svm
import matplotlib.ticker as mticker
from matplotlib.ticker import MaxNLocator,IndexFormatter
from sklearn.ensemble import RandomForestClassifier
from matplotlib.colors import LinearSegmentedColormap
from sklearn.cluster import DBSCAN
from progress.bar import Bar


def SVDC_deploy(df, period_of_interest, variables=['precip', 'temp'], add_runoff_binary=False, prediction_year=2012, epidemic_classification_dict=None, first_training_year=2000, t0_vector=None, p_vector=None, classifier='forest', modes=[0,1], decision_map=None, decision_coordinates=None, decision_values=None, clustering=True, verbose=False, tiebreaker=True):
    '''
    SVD_decision_ensemble performs a decision ensemble based on a series of decision groups generated by
    a clustering analysis.

    #Clustering process. After
    - p_max, p_min: sets the bounds for the period length vector
    - period_of_interest = () #initial and final date that contains the period of interest (poi).
    the period of interest defines the starting and finishing dates for the SVD classifierself.
    e.g. If poi is 01-02-YYYY through 28-02-YYYY, SVD classifier's heatmap will start on 28-02 of previous year and end
    on 01-02 of the next year
    -prediction_year
    -epidemic_classification_dict = dictionary. e.g. {'2001':1, '2002':0, '2003':1}
    '''
    print('Starting SVDC with following parameters:')
    print('Features: {0}'.format(variables))
    print('SVD modes : {0}'.format(modes))
    print('runoff_binary : {0}'.format(add_runoff_binary))
    print('Classifier : {0}'.format(classifier))
    time.sleep(3)
    #Find decision groups
    decision_value_min = decision_values[0]
    decision_value_max = decision_values[1]
    if verbose:
        print('Decision values {0}, {1}'.format(decision_value_min, decision_value_max))
        print('decision_map min and max values: {0}, {1}.'.format(np.min(decision_map), np.max(decision_map)))
        print('MODES:{0}'.format(modes))

    #turns a nxm decision map into a set of samples.
    # matrix = bidimensional numpy array
    if verbose:
        print("Plotting decision map. Please verify everything's correct.")
        a = plt.subplot(1,1,1)
        a_im=a.matshow(decision_map, cmap=plt.cm.hot, aspect='auto', origin='lower')
        plt.ylabel('Decision Map')
        a.yaxis.set_label_position("right")
        a.xaxis.tick_bottom()
        plt.colorbar(a_im, ax=a)
        a.set_xticks(list(range(len(decision_coordinates[0]))), minor=False)
        a.xaxis.set_major_formatter(IndexFormatter(decision_coordinates[0]))
        a.xaxis.set_major_locator(mticker.MaxNLocator(8))
        plt.xticks(rotation=40)
        a.xaxis.set_major_locator(mticker.MaxNLocator(5))
        a.axes.get_xaxis().set_visible(False)
        #plt.show()
        plt.close()


    decision_map[~((decision_map >= decision_value_min) & (decision_map <= decision_value_max))] =0
    rows, columns = np.where((decision_map > 0 ))
    roi = np.vstack([rows,columns]).T


    cluster_weights = []
    cluster_coordinates = []
    cluster_t0 = []
    cluster_p =  []
    total_value_sum = 0



    if clustering:

        #We use a clustering algorithm to find the decision clusters
        db=DBSCAN(eps=5, min_samples=40).fit(roi)
        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
        core_samples_mask[db.core_sample_indices_] = True
        labels = db.labels_

        # Find coordinates for decision clusters

        indices = [i for i,label in enumerate(labels) if label > -1]
        n_clusters = np.max(labels)+1

        for cluster_number in range(n_clusters):

            cluster_mask = np.equal(labels, cluster_number)
            cluster_indices = [i for i, val in enumerate(cluster_mask) if val==1]

            cluster_sum = 0

            for ind in cluster_indices: # For each sample within cluster

                #get sample coordinates within decision_map
                p_coordinate = roi[ind,0]
                t0_coordinate = roi[ind, 1]

                # get t0 and p coordinates
                cluster_t0.append(decision_coordinates[0][t0_coordinate])
                cluster_p.append(decision_coordinates[1][p_coordinate])
                cluster_sum+=decision_map[p_coordinate,t0_coordinate]

            total_value_sum += cluster_sum
            cluster_coordinates.append(cluster_indices)
            cluster_weights.append(cluster_sum)
    else:
        #If clustering is set to false, we grab all the regions as one big cluster
        cluster_indices = list(range(len(rows)))
        cluster_sum = 0
        n_clusters = 1
        labels = np.array([1]*len(rows))
        if len(cluster_indices)%2 == 0:
            cluster_indices.pop()
        for ind in cluster_indices: # For each sample within cluster
            #get sample coordinates within decision_map
            p_coordinate = roi[ind,0]
            t0_coordinate = roi[ind, 1]

            # get t0 and p coordinates
            cluster_t0.append(decision_coordinates[0][t0_coordinate])
            cluster_p.append(decision_coordinates[1][p_coordinate])
            cluster_sum+=decision_map[p_coordinate,t0_coordinate]

        total_value_sum += cluster_sum
        cluster_coordinates.append(cluster_indices)
        cluster_weights.append(cluster_sum)

    cluster_weights = np.array(cluster_weights)/total_value_sum #Normalizing
    all_indices = np.hstack(cluster_coordinates)

    if verbose:
        print('{0} decision clusters were found'.format(np.max(labels+1)))
        print('Plotting clustered grid, displaying only areas of interest and clusters with different colors')
        clustered_grid = np.zeros_like(decision_map)
        for i,label in enumerate(labels):
            clustered_grid[roi[i,0],roi[i,1]]=label+1
        fig = plt.figure()
        a= plt.subplot(2,1,1)
        a.matshow(decision_map, cmap=plt.cm.hot, aspect='auto', origin='lower')
        #a.colorbar()
        plt.title('Original decision_map')
        b=plt.subplot(2,1,2)
        b.matshow(clustered_grid, cmap=plt.cm.hot, aspect='auto', origin='lower', vmin=0, vmax=np.max(labels)+1) # pl is pylab imported a pl plt.cm.hot
        #a.colorbar()
        plt.title('Clusters with classifying acc {0} to {1}'.format(decision_value_min, decision_value_max))
        #plt.show()
        time.sleep(1)
        plt.close()


    if verbose:
        print('Cluster_weights = {0}'.format(cluster_weights))
        print('cluster_coordinates = {0}'.format(cluster_coordinates))
        print('all_indices {0}'.format(all_indices[:]))

    #Generate grid based on p and t0 vectors
    distance_grid = np.zeros_like(decision_map)
    years = []
    for i in range(df.index.shape[0]):
        years.append(df.index[i].year)
    years = sorted(list(set(years)))

    if verbose:
        print(years)

    if prediction_year in years and first_training_year in years:
        training_years = years[years.index(first_training_year): years.index(prediction_year)]
        n_years = len(training_years)
    else:
        print('Missing either prediction_year or first_training_year')
        time.sleep(10)
        return

    '''
    years_before_prediction = years.index(prediction_year)


    training_years = years[0:years_before_prediction]
    n_years = years_before_prediction
    '''


    if verbose:
        print('{0} years detected within dataframe: {1}.'.format(len(years), years))
        print('{0} Years before prediction: {1}'.format(n_years, training_years))

    # check if t0 dates are within poi
    dates_within_poi=[]
    for d in cluster_t0:
        if '{0}'.format(prediction_year) + d[4:] in df[period_of_interest[0]:period_of_interest[1]].index:
            dates_within_poi.append(d)

    if len(dates_within_poi) > 0:
        print('{0} dates from t0_vector are inside period_of_interest range: {1}'.format(len(dates_within_poi),dates_within_poi))


    #Enter main loop
    print('Initiating heatmap loop.')
    bar = Bar('Processing', max=len(cluster_p))
    for p, t0, ind in zip(cluster_p, cluster_t0, all_indices):
        bar.next()
        i = roi[ind,0]
        j = roi[ind,1]

        if verbose: print('Reshaping data')
        X = SVDC_reshape_yearly_data_stolerman(df=df[variables], t0=t0, p=p,\
                                               years=training_years, \
                                               upper_bound=period_of_interest[0],\
                                               normalize=True, verbose=False)

        if verbose: print('Reshaping data done')

        '''
        Each column of X represents one year of data in the order of years_before_prediction. If we want out classification at year Y
        we need Y-1 as out of sample input and Y-2, Y-3...1 as our training dataset. As we're trying to classify every Y with previous year data, we also assign
        the epidemic classification of year Y to the label for Y-1
        '''
        if X is not None:

            X_train = X[:,:-1]
            X_predict = X[:,-1]
            Y_train = []
            for year in training_years[:-1]: # Can take out of loop but keeping for clear reading purposes
                Y_train.append(epidemic_classification_dict[year+1])

            Y_train=np.vstack(Y_train)

            # Perform svd
            U, sigma, VT = svd(X_train, n_components =3, n_iter=15, random_state=None)
            U, sigma, VT = svd(X_train, n_components =3, n_iter=15, random_state=None)
            projections = sigma.reshape([-1,1])*VT
            projections = projections.T

            projections = np.vstack([projections[:,modes], np.matmul(X_predict.reshape([1,-1]),U[:,modes])])

            if add_runoff_binary:
                # This function returns the delta value stated in Stolerman's paper
                average_runoff = SVDC_get_runoffbinary(df=df, t0=t0, p=p,\
                                                       years=training_years, \
                                                       upper_bound=period_of_interest[0],\
                                                       normalize=True, verbose=False)
                classifier_dataset = np.hstack([projections, average_runoff])
            else:
                classifier_dataset = projections

            classifier_dataset_train = classifier_dataset[:-1,:]
            classifier_dataset_predict = classifier_dataset[-1,:]

            '''
            Now that we got our projections from SVD we can create the classifier
            '''
            if classifier == 'svm':
                mod = svm.SVC(kernel='rbf', gamma=1, C=1, cache_size=400, max_iter=100000)

            elif classifier == 'forest':
                mod = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=0)

            if verbose:
                print('Fitting with projections shape {0}'.format(projections.shape))
                print(Y_train, training_years)

            mod.fit(classifier_dataset_train, Y_train.ravel())
            pred = mod.predict(classifier_dataset_predict.reshape(1,-1))
            distance_grid[i,j] = pred

    bar.finish()
    cluster_decisions = []
    for cluster_number in range(n_clusters):
        accumulated_decision = 0
        indices = cluster_coordinates[cluster_number]

        for p_coordinate, t0_coordinate in roi[indices,:]:
            accumulated_decision += distance_grid[p_coordinate,t0_coordinate]*decision_map[p_coordinate, t0_coordinate] #Decision weighted by classifier accuracy

        if accumulated_decision > 0:
            cluster_decisions.append(1)
        elif accumulated_decision < 0:
            cluster_decisions.append(-1)
        else:
            cluster_decisions.append(0)


    cluster_decisions = np.array(cluster_decisions)
    final_decision = np.sum(cluster_decisions*cluster_weights)


    if verbose:
        fig, axarr = plt.subplots(3,1,figsize=[4.5,10])

        #axarr[0]= plt.subplot(3,1,1)
        a_im=axarr[0].matshow(decision_map, cmap=plt.cm.hot, aspect='auto', origin='lower', vmin=0, vmax=1)
        axarr[0].axes.get_xaxis().set_visible(False)
        plt.colorbar(a_im, ax=axarr[0])
        axarr[0].set_title('Decision map (Year to predict = {2}) \n (classifying acc {0} to {1})'.format(decision_value_min, decision_value_max, prediction_year))
        axarr[0].yaxis.set_label_position("right")
        #b=plt.subplot(3,1,2)
        b_im=axarr[1].matshow(clustered_grid, cmap=plt.cm.tab20c, aspect='auto', origin='lower', vmin=0, vmax=np.max(labels)+1) # pl is pylab imported a pl plt.cm.hot
        plt.colorbar(b_im, ax=axarr[1])
        axarr[1].axes.get_xaxis().set_visible(False)
        axarr[1].yaxis.set_label_position("right")
        axarr[1].set_title('Clusters (N={0})'.format(n_clusters))
        #c=plt.subplot(3,1,3)
        c_im=axarr[2].matshow(distance_grid, cmap=plt.cm.hot, aspect='auto', origin='lower', vmin=-1, vmax=1) # pl is pylab imported a pl plt.cm.hot
        plt.colorbar(c_im, ax=axarr[2])
        axarr[2].set_title('Cluster decisions (final={0})'.format(final_decision))
        axarr[2].yaxis.set_label_position("right")
        axarr[2].set_xticks(list(range(len(decision_coordinates[0]))), minor=False)
        axarr[2].xaxis.set_major_formatter(IndexFormatter(decision_coordinates[0]))
        axarr[2].xaxis.set_major_locator(mticker.MaxNLocator(8))
        axarr[2].xaxis.tick_bottom()
        plt.xticks(rotation=40)
        plt.close()
    else:
        axarr=None
        fig=None

    votes_against=np.sum(distance_grid[distance_grid==-1])
    votes_favor = np.sum(distance_grid[distance_grid==1])
    total_votes = len(cluster_p)

    if verbose:
        print('Decision ensemble finished with the following votes for each cluster (1 in favor, -1 against) \n')
        for c in range(n_clusters):
            print('Cluster {0} = {1}'.format(c+1, cluster_decisions[c]))
        print('Puntual Decision (decision for each classifier) distribution. \n \
              {0} in favor ({1}%).\n {2} against ({3}%). \n Total votes {4}'.format(votes_favor, votes_favor/total_votes, votes_against, votes_against/total_votes,total_votes))

    return final_decision, cluster_decisions, cluster_weights, distance_grid, fig
# Get periods

def generate_date_vector(years, months, days):

    date_vector = []
    for i in years:
        for j in months:
            for k in days:
                if k < 10 and j < 10:
                    date_vector.append('{0}-0{1}-0{2}'.format(i, j, k))
                elif k >= 10 and j < 10:
                    date_vector.append('{0}-0{1}-{2}'.format(i, j, k))
                elif k < 10 and j >= 10:
                    date_vector.append('{0}-{1}-0{2}'.format(i, j, k))
                elif k >= 10 and j >= 10:
                    date_vector.append('{0}-{1}-{2}'.format(i, j, k))

    return date_vector



def average_peak_frequency(vector, verbose=False):
    peak_indices = detect_peaks(vector, mph=0, mpd=1, threshold=0, edge='rising',
                     kpsh=False, valley=False, show=False, ax=None)

    if len(peak_indices)>0:
        delta = 1/np.mean(np.hstack([peak_indices[0],np.diff(peak_indices)])) # inverse Average distance (days) between precipitation peaks
    else:
        delta = 0
    return delta

def n_runoff_absorption(vector, verbose =False):

    return np.sum(np.equal(vector,0))


def map_timeseries_to_points(df=None, t0='2002-03-01', p=40, n_years=11,\
                             operation_dict=None, operation_per_variable=None, \
                             year_epidemic_classification=None, normalize=False, verbose=False):

    # Converts meteorological timeseries based on a specific operations for each variable
    y = int(t0[0:4])
    samples = []
    for i in range(0,n_years):
        values = []
        t = '{0}'.format(y+i) + t0[4:]

        ind = df.index.get_loc(t)
        timeseries = df[ind:ind+p]

        if verbose:
            print(timeseries)

        if normalize:
            timeseries = standardize_df(timeseries)

        if verbose:
            print(timeseries)

        for variable, operation in operation_per_variable.items():
            values.append(operation_dict[operation](timeseries[variable].values))

        values.append(year_epidemic_classification[i])
        samples.append(values)
    X = np.vstack(samples)

    return X


def calculate_subgrid_ranges(p_vector=None, t0_vector=None, subgrid_size=(6,5)):
    # subgrid_size = (t0_subsize, p_subsize)
    t0_divisions = len(t0_vector)//subgrid_size[1]
    p_divisions = len(p_vector)//subgrid_size[0]
    ranges = []


    for i in range(t0_divisions):
        for j in range(p_divisions):
            ranges.append( [ (i*subgrid_size[1], (i+1)*subgrid_size[1]) , (j*subgrid_size[0], (j+1)*subgrid_size[0])  ] )

    return ranges

def generate_samples(df=None, p_vector=None, t0_vector=None, subgrid_range=None, \
                     year_epidemic_classification=None, operation_dict=None,\
                     operation_per_variable=None):
    sub_datasets = []
    for i in range(subgrid_range[0][0], subgrid_range[0][1]):
        for j in range(subgrid_range[1][0], subgrid_range[1][1]):

            sub_dataset = map_timeseries_to_points(df=df, t0=t0_vector[i], p=p_vector[j], n_years=11,\
                                             operation_dict=operation_dict, operation_per_variable=operation_per_variable, \
                                             year_epidemic_classification=year_epidemic_classification)
            sub_datasets.append(sub_dataset)

    X = np.vstack(sub_datasets)
    return X

def plot_region(labeled_samples=None, class_color=None, verbose=True):

    for i in range(np.size(labeled_samples, axis=0)):
        print()
        plt.scatter(labeled_samples[i,0],labeled_samples[i,1], c=class_color[labeled_samples[i,2]])

    plt.show()


def standardize_df(df=None):
    #Normalize each column of a dataframe
    df_names = list(df)

    for name in df_names:
        mu = df[name].values.mean()
        sigma = df[name].values.std()
        df[name] = (df[name]-mu)/sigma
    return df

def standardize_matrix(matrix=None):

    mu = matrix.mean(axis=0)
    sigma = matrix.std(axis=0)
    matrix -= mu
    matrix /= sigma

    return matrix

def make_meshgrid(x, y, h=.02, space=.2):
    """Create a mesh of points to plot in

    Parameters
    ----------
    x: data to base x-axis meshgrid on
    y: data to base y-axis meshgrid on
    h: stepsize for meshgrid, optional

    Returns
    -------
    xx, yy : ndarray
    """
    x_d = x.max()-x.min()
    y_d = y.max()-y.min()
    x_min, x_max = x.min() - x_d*space, x.max() + x_d*space
    y_min, y_max = y.min() - y_d*space, y.max() + y_d*space
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    """Plot the decision boundaries for a classifier.

    Parameters
    ----------
    ax: matplotlib axes object
    clf: a classifier
    xx: meshgrid ndarray
    yy: meshgrid ndarray
    params: dictionary of params to pass to contourf, optional
    """
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

def svm_test(labeled_samples=None, class_color=None, C=1):

    X0, X1, labels = labeled_samples[:, 0], labeled_samples[:, 1], labeled_samples[:,2]
    x, y = make_meshgrid(X0, X1)
    models = (svm.SVC(kernel='linear', C=C),
              svm.SVC(kernel='rbf', gamma=0.7, C=C))
    models = (m.fit(labeled_samples[:,:2], labeled_samples[:,2]) for m in models)

    titles = ('SVC with linear kernel',
              'SVC with RBF kernel')

    fig, sub = plt.subplots(1, 2)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    for clf, title, ax in zip(models, titles, sub.flatten()):
        plot_contours(ax, clf, x, y,
                      cmap=plt.cm.coolwarm, alpha=0.8)
        ax.scatter(X0, X1, c=labels, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
        ax.set_xlim(x.min(), x.max())
        ax.set_ylim(y.min(), y.max())
        ax.set_xlabel('Peak Frequency')
        ax.set_ylabel('Average Temperatue')
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(title)

    plt.show()
    plt.close()

def test_accuracy(mod,validation_dataset):
    X_pred = validation_dataset[:,:2]
    labels = validation_dataset[:,2]
    z = mod.predict(X_pred)
    acc = np.sum(z==labels)/len(z)
    return acc

def separate_dataset(labeled_samples, training=.8):
    n_samples = np.size(labeled_samples,axis=0)
    n_training = int(n_samples*.8)

    randomized_indices = np.random.permutation(list(range(n_samples)))
    training_dataset = labeled_samples[randomized_indices[0:n_training],:]
    validation_dataset = labeled_samples[randomized_indices[n_training:],:]

    return training_dataset, validation_dataset

def loop_svm(labeled_samples=None, n_times=100, kernel='rbf'):
    acc = []

    for i in range(n_times):
        mod = svm.SVC(kernel=kernel, gamma=1, C=1, cache_size=400, max_iter=100000)
        training_dataset, validation_dataset = separate_dataset(labeled_samples)


        mu = np.mean(validation_dataset[:,:2], axis=0)
        sigma = np.std(training_dataset[:,:2], axis=0)
        training_dataset[:,:2] = (training_dataset[:,:2]-mu)/sigma
        validation_dataset[:,:2] = (validation_dataset[:,:2] - mu)/sigma

        mod.fit(training_dataset[:,:2], training_dataset[:,2])
        acc.append(test_accuracy(mod, validation_dataset))

    acc = np.array(acc).sum()/len(acc)

    return acc




operation_dict = {
    'average':np.mean,
    'average_peak_frequency':average_peak_frequency
}

operation_per_variable = {
    'Precipitacao':'average_peak_frequency',
    'Temp Comp Media':'average'
}



# Get periods

def ggh_yearly_data(df=None, t0='2002-03-01', p=40, n_years=11, normalize=True):

    period_matrix = []
    y = int(t0[0:4])
    for i in range(0,n_years):
        t = '{0}'.format(y+i) + t0[4:]

        ind = df.index.get_loc(t)
        timeseries = df[ind:ind+p]

        timeseries =  timeseries.T.stack().reset_index(name='new')['new'].values
        timeseries = timeseries.reshape([-1,1])
        period_matrix.append(timeseries)

    period_matrix = np.hstack(period_matrix)
    if normalize:
        period_matrix = standardize_matrix(period_matrix)
    return period_matrix


def reshape_yearly_data_stolerman(df=None, t0='2002-03-01', p=40, n_years=11, normalize=True):

    period_matrix = []
    timeseries_list = []
    indices = []
    y = int(t0[0:4])
    for i in range(0,n_years):
        t = '{0}'.format(y+i) + t0[4:]

        ind = df.index.get_loc(t)
        timeseries_list.append(df[ind:ind+p])

    stacked_timeseries = pd.concat(timeseries_list, axis=0)
    if normalize:
        stacked_timeseries = standardize_df(stacked_timeseries)

    for i in range(0,n_years):

        t = '{0}'.format(y+i) + t0[4:]

        ind = stacked_timeseries.index.get_loc(t)
        timeseries = stacked_timeseries[ind:ind+p]
        timeseries =  timeseries.T.stack().reset_index(name='new')['new'].values
        timeseries = timeseries.reshape([-1,1])
        period_matrix.append(timeseries)

    period_matrix = np.hstack(period_matrix)
    return period_matrix

def SVDC_reshape_yearly_data_stolerman(df=None, t0='2002-03-01', p=40, years=None,\
                                       upper_bound=None, normalize=True, verbose=False):

    if verbose:
        print('starting functions with following parameters: df {0},\n t0={1},\n p={2},\n years = {3},\n upper_bound={4},\n normalize={5}'.format(df, t0, p, years, upper_bound, normalize))
    period_matrix = []
    timeseries_list = []
    indices = []
    y = int(t0[0:4])
    dif = y-years[0]

    if verbose: print(t0, p, dif)
    for year in years:
        if verbose: print(year)


        t = '{0}'.format(year+dif) + t0[4:]


        ind = df.index.get_loc(t)

        if verbose:
            print(t, t0)
            print('finding index passed:{0}'.format(ind))

        if upper_bound: # Exclude any period that included dates within the period of interest

            if df[ind+p:ind+p+1].index[0] in df[upper_bound:].index:
                if verbose:
                    print('Crossed upper bound, exiting')
                return None

        timeseries_list.append(df[ind:ind+p])
        if verbose: print('Succesfully appended data of shape {0}'.format(df[ind:ind+p].shape))
    stacked_timeseries = pd.concat(timeseries_list, axis=0)


    if normalize:
        stacked_timeseries = standardize_df(stacked_timeseries)
    if verbose: print('Successfully normalized. Entering retrieval loop')


    for year in years:
        t = '{0}'.format(year+dif) + t0[4:]

        ind = stacked_timeseries.index.get_loc(t)

        if verbose:
            print(t, ind)

        timeseries = stacked_timeseries[ind:ind+p]
        timeseries =  timeseries.T.stack().reset_index(name='new')['new'].values
        timeseries = timeseries.reshape([-1,1])
        period_matrix.append(timeseries)

    period_matrix = np.hstack(period_matrix)

    return period_matrix


def SVDC_get_runoffbinary(df=None, t0='2002-03-01', p=40, years=None, var_name='runoff_binary',\
                                       upper_bound=None, normalize=True, verbose=False):

    list_apfs= []

    if verbose:
        print('starting function "get_runoffbinary" with following parameters: df {0},\n t0={1},\n p={2},\n years = {3},\n upper_bound={4},\n normalize={5}'.format(df, t0, p, years, upper_bound, normalize))
    runoff_frequencies = []
    indices = []
    y = int(t0[0:4])
    dif = y-years[0]
    timeseries_list =[]
    if verbose: print(t0, p, dif)
    for year in years:
        if verbose: print(year)


        t = '{0}'.format(year+dif) + t0[4:]


        ind = df.index.get_loc(t)

        if verbose:
            print(t, t0)
            print('finding index passed:{0}'.format(ind))

        if upper_bound: # Exclude any period that included dates within the period of interest

            if df[ind+p:ind+p+1].index[0] in df[upper_bound:].index:
                if verbose:
                    print('Crossed upper bound, exiting')
                return None

        timeseries=df[ind:ind+p][var_name].values
        apf = n_runoff_absorption(timeseries)
        if verbose:
            st ='afp(['
            for i, v in enumerate(timeseries):
                if i < len(timeseries) -1 :
                    st += '{0},'.format(v)
                else:
                    st += '{0}])'.format(v)

            print('MATLAB command = {0}'.format(st))
            print('Successfully computed average peaks. timeseries={0} \n\n peaks={1}'.format(timeseries, apf))
            plt.plot(timeseries)
            plt.title('Precipitacao. apf={0}'.format(apf))
            plt.xlabel('Time')
            plt.show()
            plt.close()
        runoff_frequencies.append(apf)

    runoff_frequencies = np.vstack(runoff_frequencies)
    return runoff_frequencies





def SVDC_get_apfs(df=None, t0='2002-03-01', p=40, years=None, var_name='Precipitacao',\
                                       upper_bound=None, normalize=True, verbose=False):

    list_apfs= []

    if verbose:
        print('starting function "get_peak_frequencies" with following parameters: df {0},\n t0={1},\n p={2},\n years = {3},\n upper_bound={4},\n normalize={5}'.format(df, t0, p, years, upper_bound, normalize))
    average_peak_frequencies = []
    indices = []
    y = int(t0[0:4])
    dif = y-years[0]
    timeseries_list =[]
    if verbose: print(t0, p, dif)
    for year in years:
        if verbose: print(year)


        t = '{0}'.format(year+dif) + t0[4:]


        ind = df.index.get_loc(t)

        if verbose:
            print(t, t0)
            print('finding index passed:{0}'.format(ind))

        if upper_bound: # Exclude any period that included dates within the period of interest

            if df[ind+p:ind+p+1].index[0] in df[upper_bound:].index:
                if verbose:
                    print('Crossed upper bound, exiting')
                return None

        timeseries=df[ind:ind+p][var_name].values
        apf = average_peak_frequency(timeseries)
        if verbose:
            st ='afp(['
            for i, v in enumerate(timeseries):
                if i < len(timeseries) -1 :
                    st += '{0},'.format(v)
                else:
                    st += '{0}])'.format(v)

            print('MATLAB command = {0}'.format(st))
            print('Successfully computed average peaks. timeseries={0} \n\n peaks={1}'.format(timeseries, apf))
            plt.plot(timeseries)
            plt.title('Precipitacao. apf={0}'.format(apf))
            plt.xlabel('Time')
            plt.show()
            plt.close()
        average_peak_frequencies.append(apf)

    average_peak_frequencies = np.vstack(average_peak_frequencies)
    return average_peak_frequencies

def standardize_df(df=None):
    #Normalize each column of a dataframe
    df_names = list(df)

    for name in df_names:
        mu = df[name].values.mean()
        sigma = df[name].values.std()
        df[name] = (df[name]-mu)/sigma
    return df

def standardize_matrix(matrix=None):

    mu = matrix.mean(axis=0)
    sigma = matrix.std(axis=0)
    matrix -= mu
    matrix /= sigma

    return matrix


def get_hulls(projections, year_epidemic_classification, verbose = False):

    yes = []
    no = []
    coordinates_1 = []
    coordinates_2 = []

    for i, v in enumerate(year_epidemic_classification):
        if v == 0:
            no.append(v)
            coordinates_2.append(projections[i,:])
        else:
            yes.append(v)
            coordinates_1.append(projections[i,:])

    coordinates_2 = np.vstack(coordinates_2)
    coordinates_1 = np.stack(coordinates_1)
    hull_1 =ConvexHull(coordinates_1)
    hull_2 = ConvexHull(coordinates_2)

    if verbose:
            plt.plot(projections[:,0], projections[:,1], 'o')
            for simplex in hull_1.simplices:
                plt.plot(coordinates_1[simplex, 0], coordinates_1[simplex, 1], 'k-')
            for simplex in hull_2.simplices:
                plt.plot(coordinates_2[simplex, 0], coordinates_2[simplex, 1], 'k-', color='r')
            plt.show()

    return hull_1, hull_2, coordinates_1, coordinates_2

def plot_hulls(projections, hull_1, hull_2, coordinates_1, coordinates_2, year_epidemic_classification, class_color):

    for i, label in enumerate(year_epidemic_classification):
        plt.plot(projections[i,0], projections[i,1], 'o', color = class_color[label])

    for simplex in hull_1.simplices:
        plt.plot(coordinates_1[simplex, 0], coordinates_1[simplex, 1], 'k-', color='b')
        plt.plot(coordinates_1[simplex, 0], coordinates_1[simplex, 1], 'o', color='b')
    for simplex in hull_2.simplices:
        plt.plot(coordinates_2[simplex, 0], coordinates_2[simplex, 1], 'k-', color='r')
        plt.plot(coordinates_2[simplex, 0], coordinates_2[simplex, 1], 'o', color='r')
    plt.show()
def plot_polygon(projections, hull_1, hull_2, coordinates_1, coordinates_2, year_epidemic_classification, class_color):

    y = np.vstack([coordinates_1[hull_1.vertices,:], coordinates_1[hull_1.vertices[0],:]])
    n = np.vstack([coordinates_2[hull_2.vertices,:], coordinates_2[hull_2.vertices[0],:]])

    for i, label in enumerate(year_epidemic_classification):
        plt.plot(projections[i,0], projections[i,1], 'o', color = class_color[label])

    plt.plot(n[:,0], n[:,1],color='r')
    plt.plot(y[:,0], y[:,1],color='b')
    plt.show()

def intersect_hulls(hull_1, hull_2, coordinates_1, coordinates_2):

    vertices_1 = np.vstack([coordinates_1[hull_1.vertices,:], coordinates_1[hull_1.vertices[0],:]])
    vertices_2 = np.vstack([coordinates_2[hull_2.vertices,:], coordinates_2[hull_2.vertices[0],:]])

    polygon_1=Polygon(vertices_1)
    polygon_2=Polygon(vertices_2)

    return polygon_1.intersects(polygon_2), vertices_1, vertices_2

def get_hull_distance( intersect_boolean, vertices_1, vertices_2):

    if intersect_boolean:
        v_distance = 0
    else:
        hull_1_coordinates = np.vstack(vertices_1)
        hull_2_coodinates = np.vstack(vertices_2)
        v_distance = np.min(distance.cdist(hull_1_coordinates,hull_2_coodinates).min(axis=1))
    return v_distance
